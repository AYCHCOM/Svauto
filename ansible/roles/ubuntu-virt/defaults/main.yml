# Copyright 2018, TCMC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

setup: "null"

mode: "null"

pod: "null"

stack_name: "null"

enable_multiqueue: 'no'
number_of_queues: '0'

libvirt_default_network_name: "default"
libvirt_default_pool_name: "default"

libvirt_pool_1: /var/lib/libvirt/images

## VMs definitions
vms:

- name: svauto-{{ controller_sequence }}
  description: "SVAuto Ubuntu Desktop, for Ansible, IPMI Access and Virt-Manager"
  firmware: efi
  memory: 2049
  cpu: 4
  disk_size: 8
  devices:
    disks:
    - name: svauto-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

- name: vnft-{{ controller_sequence }}
  description: "L3 Router with nftables"
  firmware: efi
  memory: 2049
  cpu: 2
  disk_size: 8
  devices:
    disks:
    - name: vunft-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

#
# VMs for OpenStack and Ceph via "OpenStack Ansible" project
#

- name: vosa-{{ controller_sequence }}
  description: "OpenStack and Ceph Ansible Server"
  firmware: efi
  memory: 8192
  cpu: 4
  disk_size: 20
  devices:
    disks:
    - name: vosa-{{ controller_sequence }}
      size: 20
      driver:
        type: qcow2

- name: vosctrl-{{ controller_sequence }}
  description: "OpenStack Controller 1"
  firmware: efi
  memory: 32768
  cpu: 16
  disk_size: 128
  devices:
    disks:
    - name: vosctrl-{{ controller_sequence }}
      size: 108
      driver:
        type: qcow2

- name: voscmpt-{{ controller_sequence }}
  description: "OpenStack LXD Virtual Compute 1"
  firmware: efi
  memory: 65536
  cpu: 32
  disk_size: 20
  devices:
    disks:
    - name: voscmpt-{{ controller_sequence }}
      size: 25
      driver:
        type: qcow2

- name: vceinfr-{{ controller_sequence }}
  description: "Ceph Infrastructure, Monitor and Manager"
  firmware: efi
  memory: 8192
  cpu: 4
  disk_size: 8
  devices:
    disks:
    - name: vceinfr-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

- name: vcerdgw-{{ controller_sequence }}
  description: "Ceph Rados Gateway"
  firmware: efi
  memory: 8192
  cpu: 4
  disk_size: 8
  devices:
    disks:
    - name: vcerdgw-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

- name: vcemeta-{{ controller_sequence }}
  description: "Ceph Metadata"
  firmware: efi
  memory: 8192
  cpu: 4
  disk_size: 8
  devices:
    disks:
    - name: vcemeta-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

#
# Isolated Ceph Infrastructure (not installed via `openstack-ansible`)
#

- name: vuci-{{ controller_sequence }}
  description: "Ceph Infrastructure, Monitor and Manager"
  firmware: efi
  memory: 8192
  cpu: 4
  disk_size: 8
  devices:
    disks:
    - name: vuci-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

- name: vucr-{{ controller_sequence }}
  description: "Ceph Rados Gateway"
  firmware: efi
  memory: 8192
  cpu: 4
  disk_size: 8
  devices:
    disks:
    - name: vucr-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

- name: vucm-{{ controller_sequence }}
  description: "Ceph Metadata"
  firmware: efi
  memory: 8192
  cpu: 4
  disk_size: 8
  devices:
    disks:
    - name: vucm-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

- name: vuco-{{ controller_sequence }}
  description: "Ceph Virtual OSD for tests"
  firmware: efi
  memory: 8192
  cpu: 4
  disk_size: 8
  devices:
    disks:
    - name: vuco-{{ controller_sequence }}
      size: 8
      driver:
        type: qcow2

# NOTE: It makes more sense to host those VMs on OpenStack.
# Each of those will become a "Stack", a Heat Template.

#- name: viscsit-{{ controller_sequence }}
#  description: "iSCSI Targets"
#  firmware: efi
#  memory: 8192
#  cpu: 4
#  disk_size: 4
#  devices:
#    disks:
#    - name: viscsi-{{ controller_sequence }}
#      size: 4
#      driver:
#        type: qcow2
#
#- name: vnfsgane-{{ controller_sequence }}
#  description: "NFS Server"
#  firmware: efi
#  memory: 8192
#  cpu: 4
#  disk_size: 4
#  devices:
#    disks:
#    - name: vnfsgane-{{ controller_sequence }}
#      size: 4
#      driver:
#        type: qcow2
